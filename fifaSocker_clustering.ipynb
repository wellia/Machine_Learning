{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fifaSocker_clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wellia/Machine_Learning/blob/main/fifaSocker_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ljGko6N-6vf"
      },
      "source": [
        "\n",
        "## Overview\n",
        "\n",
        "Recently, [Kaggle](https://www.kaggle.com) (a data science community and competition platform) released one data set '[FIFA19](https://www.kaggle.com/karangadiya/fifa19)â€˜, which consists of 18K+ FIFA 19 player with around 90 attributes extracted from FIFA database. In this assessment task, we make it available as the data set:\n",
        "- The data set is [2020T2Data.csv](https://github.com/tulip-lab/sit742/raw/master/Assessment/2020/data/2020T2Data.csv)\n",
        "\n",
        "In this task **use Spark packages**\n",
        "\n",
        "- **Part 1**: Exploratory Data Analysis\n",
        "\n",
        "- **Part 2**: Clustering Analysis, and identify the position profiles of each cluster\n",
        "\n",
        "- **Part 3**: Classification Analysis, and evaluate the performance of different algorithms using cross validation;\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1vjEdvs-6vh"
      },
      "source": [
        "## Part 1 - What we could know about FIFA 2019 Players? \n",
        "\n",
        "### 1.0. Libraries and data files\n",
        "<a id=\"Load data\"></a>\n",
        "***\n",
        "\n",
        "Import the necessary Spark environment, and load the data set [2020T2Data.csv](https://github.com/tulip-lab/sit742/raw/master/Assessment/2020/data/2020T2Data.csv).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auVRAscVRVrI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "6f992876-86f5-4eee-f655-957e5d7c83af"
      },
      "source": [
        "!pip install wget\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os,wget\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\"\n",
        "\n",
        "link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Assessment/2020/data/2020T2Data.csv'\n",
        "DataSet = wget.download(link_to_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=8ff62aa63637b755c010c4f00bba93362bd3894a51b1716d325d5ce98c4a779d\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gxm5nc6Rz-a"
      },
      "source": [
        "import findspark\n",
        "import numpy as np\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSPbqoEGR30v"
      },
      "source": [
        "### 1.1 Data Exploration\n",
        "\n",
        "*Remind: Use **PySpark** to complete the following data processing and model building. Otherwise, you lose all marks.*\n",
        "\n",
        "<a id=\"loading\"></a>\n",
        "***\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "**Code**: \n",
        "    import the csv file as a Spark dataframe and name it as df\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuGY6lgWR7B3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba2119d4-8dc7-48eb-cf5c-2ae6f0f98c20"
      },
      "source": [
        "# Import the '2020T2Data.csv' as a Spark dataframe and name it as df\n",
        "spark = SparkSession.builder.appName('SIT742T2').getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"2020T2Data.csv\", header = True)\n",
        "df.show(5)\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+--------+-------+-------+--------------+------------------------+---------+-----------+--------------+----------+---------+--------+-------------+------------+-----------+--------------------+------+----------+------+----------+--------+---------+---------------+------------+-------+---------+-----+----------+-----------+-----------+------------+-----------+-------+---------+-------+---------+-------+-------+--------+---------+----------+-------------+-----------+------+---------+---------+-------+--------------+-------------+--------+----------+---------+-------------+----------+-----------------+\n",
            "|    ID|             Name|Age|               Photo|Nationality|                Flag|Overall|Potential|               Club|           Club Logo|value(M)|wage(K)|Special|Preferred Foot|International Reputation|Weak Foot|Skill Moves|     Work Rate| Body Type|Real Face|Position|Jersey Number|      Joined|Loaned From|Contract Valid Until|Height|Height(CM)|Weight|Weight(KG)|Crossing|Finishing|HeadingAccuracy|ShortPassing|Volleys|Dribbling|Curve|FKAccuracy|LongPassing|BallControl|Acceleration|SprintSpeed|Agility|Reactions|Balance|ShotPower|Jumping|Stamina|Strength|LongShots|Aggression|Interceptions|Positioning|Vision|Penalties|Composure|Marking|StandingTackle|SlidingTackle|GKDiving|GKHandling|GKKicking|GKPositioning|GKReflexes|Release Clause(M)|\n",
            "+------+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+--------+-------+-------+--------------+------------------------+---------+-----------+--------------+----------+---------+--------+-------------+------------+-----------+--------------------+------+----------+------+----------+--------+---------+---------------+------------+-------+---------+-----+----------+-----------+-----------+------------+-----------+-------+---------+-------+---------+-------+-------+--------+---------+----------+-------------+-----------+------+---------+---------+-------+--------------+-------------+--------+----------+---------+-------------+----------+-----------------+\n",
            "|158023|         L. Messi| 31|https://cdn.sofif...|  Argentina|https://cdn.sofif...|     94|       94|       FC Barcelona|https://cdn.sofif...|   110.5|    565|   2202|          Left|                       5|        4|          4|Medium/ Medium|     Messi|      Yes|      RF|           10| Jul 1, 2004|       null|                2021|  5.70|   173.736|159.00| 72.121128|      84|       95|             70|          90|     86|       97|   93|        94|         87|         96|          91|         86|     91|       95|     95|       85|     68|     72|      59|       94|        48|           22|         94|    94|       75|       96|     33|            28|           26|       6|        11|       15|           14|         8|            226.5|\n",
            "| 20801|Cristiano Ronaldo| 33|https://cdn.sofif...|   Portugal|https://cdn.sofif...|     94|       94|           Juventus|https://cdn.sofif...|      77|    405|   2228|         Right|                       5|        4|          5|     High/ Low|C. Ronaldo|      Yes|      ST|            7|Jul 10, 2018|       null|                2022|  6.20|   188.976|183.00| 83.007336|      84|       94|             89|          81|     87|       88|   81|        76|         77|         94|          89|         91|     87|       96|     70|       95|     95|     88|      79|       93|        63|           29|         95|    82|       85|       95|     28|            31|           23|       7|        11|       15|           14|        11|            127.1|\n",
            "|190871|        Neymar Jr| 26|https://cdn.sofif...|     Brazil|https://cdn.sofif...|     92|       93|Paris Saint-Germain|https://cdn.sofif...|   118.5|    290|   2143|         Right|                       5|        5|          5|  High/ Medium|    Neymar|      Yes|      LW|           10| Aug 3, 2017|       null|                2022|  5.90|   179.832|150.00|   68.0388|      79|       87|             62|          84|     84|       96|   88|        87|         78|         95|          94|         90|     96|       94|     84|       80|     61|     81|      49|       82|        56|           36|         89|    87|       81|       94|     27|            24|           33|       9|         9|       15|           15|        11|            228.1|\n",
            "|193080|           De Gea| 27|https://cdn.sofif...|      Spain|https://cdn.sofif...|     91|       93|  Manchester United|https://cdn.sofif...|      72|    260|   1471|         Right|                       4|        3|          1|Medium/ Medium|      Lean|      Yes|      GK|            1| Jul 1, 2011|       null|                2020|  6.40|   195.072|168.00| 76.203456|      17|       13|             21|          50|     13|       18|   21|        19|         51|         42|          57|         58|     60|       90|     43|       31|     67|     43|      64|       12|        38|           30|         12|    68|       40|       68|     15|            21|           13|      90|        85|       87|           88|        94|            138.6|\n",
            "|192985|     K. De Bruyne| 27|https://cdn.sofif...|    Belgium|https://cdn.sofif...|     91|       92|    Manchester City|https://cdn.sofif...|     102|    355|   2281|         Right|                       4|        5|          4|    High/ High|    Normal|      Yes|     RCM|            7|Aug 30, 2015|       null|                2023|  5.11|  155.7528|154.00| 69.853168|      93|       82|             55|          92|     82|       86|   85|        83|         91|         91|          78|         76|     79|       91|     77|       91|     63|     90|      75|       91|        76|           61|         87|    94|       79|       88|     68|            58|           51|      15|        13|        5|           10|        13|            196.4|\n",
            "+------+-----------------+---+--------------------+-----------+--------------------+-------+---------+-------------------+--------------------+--------+-------+-------+--------------+------------------------+---------+-----------+--------------+----------+---------+--------+-------------+------------+-----------+--------------------+------+----------+------+----------+--------+---------+---------------+------------+-------+---------+-----+----------+-----------+-----------+------------+-----------+-------+---------+-------+---------+-------+-------+--------+---------+----------+-------------+-----------+------+---------+---------+-------+--------------+-------------+--------+----------+---------+-------------+----------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Photo: string (nullable = true)\n",
            " |-- Nationality: string (nullable = true)\n",
            " |-- Flag: string (nullable = true)\n",
            " |-- Overall: string (nullable = true)\n",
            " |-- Potential: string (nullable = true)\n",
            " |-- Club: string (nullable = true)\n",
            " |-- Club Logo: string (nullable = true)\n",
            " |-- value(M): string (nullable = true)\n",
            " |-- wage(K): string (nullable = true)\n",
            " |-- Special: string (nullable = true)\n",
            " |-- Preferred Foot: string (nullable = true)\n",
            " |-- International Reputation: string (nullable = true)\n",
            " |-- Weak Foot: string (nullable = true)\n",
            " |-- Skill Moves: string (nullable = true)\n",
            " |-- Work Rate: string (nullable = true)\n",
            " |-- Body Type: string (nullable = true)\n",
            " |-- Real Face: string (nullable = true)\n",
            " |-- Position: string (nullable = true)\n",
            " |-- Jersey Number: string (nullable = true)\n",
            " |-- Joined: string (nullable = true)\n",
            " |-- Loaned From: string (nullable = true)\n",
            " |-- Contract Valid Until: string (nullable = true)\n",
            " |-- Height: string (nullable = true)\n",
            " |-- Height(CM): string (nullable = true)\n",
            " |-- Weight: string (nullable = true)\n",
            " |-- Weight(KG): string (nullable = true)\n",
            " |-- Crossing: string (nullable = true)\n",
            " |-- Finishing: string (nullable = true)\n",
            " |-- HeadingAccuracy: string (nullable = true)\n",
            " |-- ShortPassing: string (nullable = true)\n",
            " |-- Volleys: string (nullable = true)\n",
            " |-- Dribbling: string (nullable = true)\n",
            " |-- Curve: string (nullable = true)\n",
            " |-- FKAccuracy: string (nullable = true)\n",
            " |-- LongPassing: string (nullable = true)\n",
            " |-- BallControl: string (nullable = true)\n",
            " |-- Acceleration: string (nullable = true)\n",
            " |-- SprintSpeed: string (nullable = true)\n",
            " |-- Agility: string (nullable = true)\n",
            " |-- Reactions: string (nullable = true)\n",
            " |-- Balance: string (nullable = true)\n",
            " |-- ShotPower: string (nullable = true)\n",
            " |-- Jumping: string (nullable = true)\n",
            " |-- Stamina: string (nullable = true)\n",
            " |-- Strength: string (nullable = true)\n",
            " |-- LongShots: string (nullable = true)\n",
            " |-- Aggression: string (nullable = true)\n",
            " |-- Interceptions: string (nullable = true)\n",
            " |-- Positioning: string (nullable = true)\n",
            " |-- Vision: string (nullable = true)\n",
            " |-- Penalties: string (nullable = true)\n",
            " |-- Composure: string (nullable = true)\n",
            " |-- Marking: string (nullable = true)\n",
            " |-- StandingTackle: string (nullable = true)\n",
            " |-- SlidingTackle: string (nullable = true)\n",
            " |-- GKDiving: string (nullable = true)\n",
            " |-- GKHandling: string (nullable = true)\n",
            " |-- GKKicking: string (nullable = true)\n",
            " |-- GKPositioning: string (nullable = true)\n",
            " |-- GKReflexes: string (nullable = true)\n",
            " |-- Release Clause(M): string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sXsdZ2wbrAg"
      },
      "source": [
        "****\n",
        "\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "**Code**: \n",
        "    Check statistics (min, mean and max) for features on Age, Overall. Then find out the Avg Overall on Position, Avg Overall on Nationality (Sort by avg Overall on Nationality)\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "**Report**: \n",
        "    **1.1.A** Please answer questions with proper section title  '1.1.A':     \n",
        "    <ol>\n",
        "        <li> Which are the (min, mean and max) for Age </li>\n",
        "        <li> Which are the (min, mean and max) for Overall </li>\n",
        "        <li> Which position the talented player (based on Avg Overall) are playing? </li>\n",
        "        <li> Which are the top 3 countres that most likely have the genies player (based on sort of Avg Overall) and </li>\n",
        "    </ol>\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdWVric-aY-h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c853e010-b954-48ff-ece9-d45449c0a5cb"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "#Statistics on Age\n",
        "df.select([F.min(\"Age\"), F.mean(\"Age\"),  F.max(\"Age\")]).show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+------------------+--------+\n",
            "|min(Age)|          avg(Age)|max(Age)|\n",
            "+--------+------------------+--------+\n",
            "|      16|25.122205745043114|      45|\n",
            "+--------+------------------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KMLB3sAfkpK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5345dd7-8b2c-45ea-f672-1026156f881c"
      },
      "source": [
        "#Statistics on Overall\n",
        "df.select([F.min(\"Overall\"), F.mean(\"Overall\"),  F.max(\"Overall\")]).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----------------+------------+\n",
            "|min(Overall)|     avg(Overall)|max(Overall)|\n",
            "+------------+-----------------+------------+\n",
            "|          46|66.23869940132916|          94|\n",
            "+------------+-----------------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4SMaNc4gX3E"
      },
      "source": [
        "# Show top position which have highest average overall\n",
        "df.groupby(['Position']).agg({\"Overall\": \"AVG\"}).sort(\"avg(Overall)\", ascending=False).show(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6PYyigEjZwI"
      },
      "source": [
        "#Top 3 countries most likely having good players\n",
        "df.groupby(['Nationality']).agg({\"Overall\": \"AVG\"}).sort(\"avg(Overall)\", ascending=False).show(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1RsTYgImPFD"
      },
      "source": [
        "# avg potentials on country by position order by country\n",
        "df_avg_potential = df.groupBy(\"Nationality\").pivot(\"Position\").agg({\"Potential\": \"AVG\"}).sort(\"Nationality\")\n",
        "print('Avg potential on country by position')\n",
        "df_avg_potential.show(10)\n",
        "\n",
        "\n",
        "# find position of top au player\n",
        "Aus_avgpot = df.filter(\"Nationality == 'Australia'\").groupby(\"Position\").agg({'Potential':'AVG'}).sort('avg(Potential)', ascending=False).show(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRvkk376u-dE"
      },
      "source": [
        "# Code for plot\n",
        "df1 = df.groupBy(\"Age\").agg({\"Potential\": \"AVG\"}).withColumnRenamed(\"avg(Potential)\", \"Average\").sort(\"Age\")\n",
        "df2 = df.groupBy(\"Age\").agg({\"Overall\": \"AVG\"}).withColumnRenamed(\"avg(Overall)\", \"Average\").sort(\"Age\")\n",
        "\n",
        "x1 = [i[0] for i in df1.select('Age').collect()]\n",
        "y1 = [i[0] for i in df1.select('Average').collect()]\n",
        "\n",
        "x2 = [i[0] for i in df2.select('Age').collect()]\n",
        "y2 = [i[0] for i in df2.select('Average').collect()]\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(x1, y1)\n",
        "plt.plot(x2, y2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMqIOge42NI7"
      },
      "source": [
        "## Part 2 - Unsupervised Learning: Kmeans\n",
        "\n",
        "<a id=\"kmeans\"></a>\n",
        "***\n",
        "\n",
        "\n",
        "### 2.1 Data Preparation\n",
        "\n",
        "*Use **pyspark** to complete the following data processing and model building.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHi1rU4p-6wK"
      },
      "source": [
        "\n",
        "****\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "**Code**: \n",
        "    You will need to remove the Goal Keepers (Position = 'GK') and only use the skillset attributes (Height(CM),\n",
        "Weight(KG),\n",
        "Crossing,\n",
        "Finishing,\n",
        "HeadingAccuracy,\n",
        "ShortPassing,\n",
        "Volleys,\n",
        "Dribbling,\n",
        "Curve,\n",
        "FKAccuracy,\n",
        "LongPassing,\n",
        "BallControl,\n",
        "Acceleration,\n",
        "SprintSpeed,\n",
        "Agility,\n",
        "Reactions,\n",
        "Balance,\n",
        "ShotPower,\n",
        "Jumping,\n",
        "Stamina,\n",
        "Strength,\n",
        "LongShots,\n",
        "Aggression,\n",
        "Interceptions,\n",
        "Positioning,\n",
        "Vision,\n",
        "Penalties,\n",
        "Composure,\n",
        "Marking,\n",
        "StandingTackle,\n",
        "SlidingTackle) \n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2LHjTMv3w_7"
      },
      "source": [
        "# Your code to select relevent features and filtering by leaving out the GK\n",
        "df = df.select('ID', 'Position', 'Height(CM)', 'Weight(KG)', 'Crossing', 'Finishing', 'HeadingAccuracy',  \n",
        "           'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', \n",
        "           'BallControl', 'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance', \n",
        "           'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', \n",
        "           'Positioning', 'Vision', 'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle').filter(\"Position != 'GK'\")\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVLBzBId9iCk"
      },
      "source": [
        "To make the later stage easier, we define the position group by using the position feature.\n",
        "- DEF = [LB,LWB,RB,LCB,RCB,CB,RWB] ,\n",
        "- FWD = [RF,LF,LW,RS,RW,LS,CF,ST] ,\n",
        "- MID = [LCM,LM,RDM,CAM,RAM,RCM,CM,CDM,RM,LAM,LDM]\n",
        "\n",
        "****\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "**Code**: \n",
        "    Create a new column called Position_Group with only DEF/FWD/MID in the dataframe you created in previously\n",
        "\n",
        "</div>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4N6edcq5kE_"
      },
      "source": [
        "from pyspark.sql.functions import when,col,lit\n",
        "\n",
        "# Define position group by position features\n",
        "DEF = ['LB','LWB','RB','LCB','RCB','CB','RWB']\n",
        "FWD = ['RF','LF','LW','RS','RW','LS','CF','ST']\n",
        "MID = ['LCM','LM','RDM','CAM','RAM','RCM','CM','CDM','RM','LAM','LDM']\n",
        "\n",
        "# create new column Position_Group with position group DEF/FWD/MD\n",
        "df_kmeans_new = df.withColumn(\"Position_Group\", when(col(\"Position\").isin(DEF), lit(\"DEF\"))\n",
        "  .when(col(\"Position\").isin(FWD), lit(\"FWD\"))\n",
        "  .otherwise(lit(\"MID\")))\n",
        "df_kmeans_new.printSchema()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-NyFp7IBtGZ"
      },
      "source": [
        "Now, we remove the Position_Group and Position to create the feature for Kmeans\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x_o_w54B-mN"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# remove Position_Group and Position to create the feature for Kmeans\n",
        "FEATURES_COL = ['Height(CM)', 'Weight(KG)', \n",
        "                      'Crossing', 'Finishing', 'HeadingAccuracy', \n",
        "                      'ShortPassing', 'Volleys', 'Dribbling', 'Curve',\n",
        "                      'FKAccuracy', 'LongPassing', 'BallControl', \n",
        "                      'Acceleration', 'SprintSpeed', 'Agility', \n",
        "                      'Reactions', 'Balance', 'ShotPower', 'Jumping', \n",
        "                      'Stamina', 'Strength', 'LongShots', 'Aggression', \n",
        "                      'Interceptions', 'Positioning', 'Vision', 'Penalties', \n",
        "                      'Composure', 'Marking', 'StandingTackle', 'SlidingTackle']\n",
        "\n",
        "for col_name in FEATURES_COL:\n",
        "    df_kmeans_new = df_kmeans_new.withColumn(col_name, col(col_name).cast('float'))\n",
        "\n",
        "vecAssembler = VectorAssembler(inputCols=FEATURES_COL, outputCol=\"features\")\n",
        "df_kmeans_ = vecAssembler.transform(df_kmeans_new).select('ID','features')\n",
        "df_kmeans_.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UniVGeYIDa_k"
      },
      "source": [
        "Now in order to evaluate your Kmeans Model, please plot the elbow plot\n",
        "\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "**Code**: \n",
        "    Plot the elbow plot, with a varying K from 2 to 20.\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "\n",
        "</div>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDia7JYyDLhg"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# draw the elbow plot varying K from 2 to 20\n",
        "cost = np.zeros(20)\n",
        "for k in range(2,20):\n",
        "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
        "    model = kmeans.fit(df_kmeans_.sample(False,0.1, seed=42))\n",
        "    cost[k] = model.computeCost(df_kmeans_) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr-bSnj5EA1l"
      },
      "source": [
        "fig, ax = plt.subplots(1,1, figsize =(6,6))\n",
        "ax.set_title('The Elbow method chart', fontsize=16)\n",
        "ax.set_xlabel('Number of clusters', fontsize=12)\n",
        "ax.set_ylabel('Cost', fontsize=12)\n",
        "ax.set_xticks(np.arange(0, 20, 2))\n",
        "ax.grid()\n",
        "ax.plot(range(2,20),cost[2:20])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XupecQFHE5Za"
      },
      "source": [
        "\n",
        "### 2.2 K-Means\n",
        "\n",
        "Could you tell out the optimized K value? \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "****\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "**Code**: \n",
        "    Choose a K value as 8 and then summarize each cluster with the count on Position_Group.\n",
        "\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxSOxcowE6vB"
      },
      "source": [
        "k = 8\n",
        "\n",
        "# Your code\n",
        "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
        "model = kmeans.fit(df_kmeans_)\n",
        "print(type(model))\n",
        "centers = model.clusterCenters()\n",
        "\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkWZ5-6oFlpp"
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "# transform id to their cluster/prediction\n",
        "modelTransformed = model.transform(df_kmeans_).select('ID', 'prediction')\n",
        "rows = modelTransformed.collect()\n",
        "\n",
        "# join prediction results to the original dataframe that contains Position_Group\n",
        "df_kmeans_pred_ = sqlContext.createDataFrame(rows)\n",
        "df_kmeans_pred_ = df_kmeans_pred_.join(df_kmeans_new, 'ID').withColumnRenamed(\"prediction\", \"Cluster\")\n",
        "df_kmeans_pred_.groupBy('Position_Group', 'Cluster').agg(count(\"*\")).sort('Position_Group', 'Cluster').show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiHEevY6IOJz"
      },
      "source": [
        "## Part 3 - Supervised Learning: Classification on Position_Group\n",
        "\n",
        "<a id=\"classification\"></a>\n",
        "***\n",
        "\n",
        "In last part, you use the player's skillset values to segment the players into 8 clusters. Check whether we could accurately predict the position_group of the player.\n",
        "\n",
        "*Ruse **PySpark** *\n",
        "\n",
        "\n",
        "### 3.1 Data Preparation\n",
        "\n",
        "We remove the feature of position and use all other skillset features and the cluster prediction as the input for the model. Your target for classification is \"Position_Group\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwtsyrUsI8e_"
      },
      "source": [
        "FEATURES_COL_ = ['Height(CM)', 'Weight(KG)', \n",
        "                      'Crossing', 'Finishing', 'HeadingAccuracy', \n",
        "                      'ShortPassing', 'Volleys', 'Dribbling', 'Curve',\n",
        "                      'FKAccuracy', 'LongPassing', 'BallControl', \n",
        "                      'Acceleration', 'SprintSpeed', 'Agility', \n",
        "                      'Reactions', 'Balance', 'ShotPower', 'Jumping', \n",
        "                      'Stamina', 'Strength', 'LongShots', 'Aggression', \n",
        "                      'Interceptions', 'Positioning', 'Vision', 'Penalties', \n",
        "                      'Composure', 'Marking', 'StandingTackle', 'SlidingTackle','Cluster']\n",
        "\n",
        "vecAssembler_ = VectorAssembler(inputCols=FEATURES_COL_, outputCol=\"features\")\n",
        "df_class_ = vecAssembler_.transform(df_kmeans_pred_).select('features','Position_Group')\n",
        "df_class_.show(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AklN81TvMKJS"
      },
      "source": [
        "In many data science modeling work, feature scaling is very important.\n",
        "In here, we use standard scaling on the fetaures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjQRAHOQMK6V"
      },
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
        "raw_data=standardscaler.fit(df_class_).transform(df_class_)\n",
        "raw_data.select(\"features\",\"Scaled_features\",'Position_Group').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD6wgq2sgiGk"
      },
      "source": [
        "In Spark, you could not use string as Target data type, Please encode the Position_Group column by using following encoding: \n",
        "\n",
        "FWD = 0\n",
        "DEF = 1\n",
        "MID = 2\n",
        "\n",
        "*Hint: Data type after encoding should be numeric.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OosmFX9iOc0Z"
      },
      "source": [
        "raw_data_ = raw_data.withColumn('Target',when(col(\"Position_Group\") == \"DEF\", 1)\n",
        "      .when(col(\"Position_Group\")== \"FWD\", 0)\n",
        "      .otherwise(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDfZARUNMW2M"
      },
      "source": [
        "### 3.2 Training Test Evaluation\n",
        "\n",
        "We remove the feature of position and use all other skillset features and the cluster prediction as the input for the model. The target for classification is \"Position_Group\".\n",
        "\n",
        "Now, we split your data into train/Test, and evaluate one model's performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmbanmwBMUQB"
      },
      "source": [
        "train, test = raw_data_.randomSplit([0.7, 0.3], seed=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apoKm9igMi12"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(labelCol=\"Target\", featuresCol=\"Scaled_features\",maxIter=10)\n",
        "model = lr.fit(train) #fit model with data\n",
        "\n",
        "# prediction on test data\n",
        "predict_test = model.transform(test)\n",
        "predict_test.select(\"Target\",\"prediction\").show(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZk-Z7CcP6Ng"
      },
      "source": [
        "\n",
        "****\n",
        "\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "**Code**: \n",
        "    You are required to evaluate the model by using confusion matrix. Please also print out your model's Precision, Recall and F1 score.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPho8CbGQCCm"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true = predict_test.select(\"Target\").toPandas()\n",
        "\n",
        "y_prediction = predict_test.select(\"prediction\").toPandas()\n",
        "\n",
        "confusionMatrix = confusion_matrix(y_true, y_prediction)\n",
        "\n",
        "print(confusionMatrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d95nOilARR8H"
      },
      "source": [
        "# Overall statistic\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "classificationReport = classification_report(y_true,y_prediction)\n",
        "print(classificationReport)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Iy-g_68Q8OV"
      },
      "source": [
        "### 3.3 K-fold Cross-Validation\n",
        "\n",
        "We surely missed something during the modeling work -- Hyperparameter tuning! We can use K-fold cross validation to find out the best hyperparameter set.\n",
        "\n",
        "****\n",
        "\n",
        "\n",
        "**Code**: \n",
        "    Implement K-fold cross validation for three (any three) classification models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yCvsy1-jO7x"
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# initialization \n",
        "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\")\n",
        "\n",
        "# -- random forest without tuning ---\n",
        "rf = RandomForestClassifier(labelCol=\"Target\", featuresCol=\"Scaled_features\")\n",
        "rf_model = rf.fit(train)\n",
        "rf_predict_test = rf_model.transform(test)\n",
        "\n",
        "rf_accuracy = multi_evaluator.evaluate(rf_predict_test, {multi_evaluator.metricName: \"accuracy\"}) #metricName can be fi, weightedPrecision, weightedRecall\n",
        "\n",
        "#Should use this paramGrid, but it is very slow\n",
        "#paramGrid = (ParamGridBuilder()\n",
        "#             .addGrid(rf.maxDepth, [5,10,20,25,30])\n",
        "#             .addGrid(rf.maxBins, [20, 60])\n",
        "#             .addGrid(rf.numTrees, [5, 20,50,100])\n",
        "#             .build())\n",
        "\n",
        "# -- random forest with tuning\n",
        "paramGrid = (ParamGridBuilder()\n",
        "               .addGrid(rf.maxDepth, [2, 4, 6])\n",
        "               .addGrid(rf.maxBins, [20, 60])\n",
        "               .addGrid(rf.numTrees, [5, 20])\n",
        "               .build())\n",
        "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=multi_evaluator, numFolds=5)\n",
        "cv_model = cv.fit(train)\n",
        "rf_model_best = cv_model.bestModel\n",
        "\n",
        "rf_predict_test_best = rf_model_best.transform(test)\n",
        "rf_accuracy_best = multi_evaluator.evaluate(rf_predict_test_best, {multi_evaluator.metricName: \"accuracy\"}) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDpCsawm3gmT"
      },
      "source": [
        "# print comparison accuracy\n",
        "print('random forest accuracy:', round(rf_accuracy, 2))\n",
        "print('random forest best model accuracy:', round(rf_accuracy_best, 2))\n",
        "\n",
        "# print parameters best model\n",
        "rf_model_best.extractParamMap()\n",
        "print('Random Forest best model params:', {param[0].name: param[1] for param in rf_model_best.extractParamMap().items()})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpDnT5qq8N16"
      },
      "source": [
        "# print comparison confusion matrix\n",
        "\n",
        "# random forest without tuning \n",
        "rf_true = rf_predict_test.select(\"Target\").toPandas()\n",
        "rf_prediction = rf_predict_test.select(\"prediction\").toPandas()\n",
        "rf_cm = confusion_matrix(rf_true, rf_prediction)\n",
        "print('Confusion matrix random forest:\\n', rf_cm)\n",
        "\n",
        "brf_true = rf_predict_test_best.select(\"Target\").toPandas()\n",
        "brf_prediction = rf_predict_test_best.select(\"prediction\").toPandas()\n",
        "brf_cm = confusion_matrix(brf_true, brf_prediction)\n",
        "print('\\nConfusion matrix random forest best model:\\n', brf_cm)\n",
        "\n",
        "# using the precision, the recall, and the f1-score to compare the random forest model and the best random forest model\n",
        "rf_overall = classification_report(rf_true, rf_prediction)\n",
        "print('\\nRandom Forest overall:\\n', rf_overall)\n",
        "brf_overall = classification_report(brf_true, brf_prediction)\n",
        "print('\\nRandom Forest best model overall:\\n', brf_overall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JFxvZIaI2VC"
      },
      "source": [
        "# Decision Trees\n",
        "\n",
        "# --- Decision tree no tuning ---\n",
        "dt = DecisionTreeClassifier(labelCol=\"Target\", featuresCol=\"Scaled_features\", maxDepth=3)\n",
        "dt_model = dt.fit(train)\n",
        "dt_predict_test = dt_model.transform(test)\n",
        "dt_accuracy = multi_evaluator.evaluate(dt_predict_test, {multi_evaluator.metricName: \"accuracy\"}) \n",
        "\n",
        "# --- Decision tree with tuning ---\n",
        "paramGrid = (ParamGridBuilder()\n",
        "               .addGrid(dt.maxDepth, [2, 4, 6])\n",
        "               .addGrid(dt.maxBins, [20, 60])\n",
        "               .build())\n",
        "\n",
        "# Create 5-fold CrossValidator\n",
        "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=multi_evaluator, numFolds=5)\n",
        "\n",
        "# Run cross validations.\n",
        "cv_model = cv.fit(train)\n",
        "dt_model_best = cv_model.bestModel\n",
        "dt_predict_test_best = dt_model_best.transform(test)\n",
        "\n",
        "dt_accuracy_best = multi_evaluator.evaluate(dt_predict_test_best, {multi_evaluator.metricName: \"accuracy\"}) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZR73wnoN4ai"
      },
      "source": [
        "# print comparison\n",
        "print('decision tree accuracy:', round(dt_accuracy, 2))\n",
        "print('decision_tree best model accuracy:', round(dt_accuracy_best, 2))\n",
        "# print parameters\n",
        "print(\"Decision tree best model params:\", {param[0].name: param[1] for param in dt_model_best.extractParamMap().items()})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlmKLDMn_7oe"
      },
      "source": [
        "# decision tree confusion matrix\n",
        "dt_true = dt_predict_test.select(\"Target\").toPandas()\n",
        "dt_prediction = dt_predict_test.select(\"prediction\").toPandas()\n",
        "dt_cm = confusion_matrix(dt_true, dt_prediction)\n",
        "print('Decision tree matrix:\\n', dt_cm)\n",
        "\n",
        "# decision tree with tuning \n",
        "bdt_true = dt_predict_test_best.select(\"Target\").toPandas()\n",
        "bdt_prediction = dt_predict_test_best.select(\"prediction\").toPandas()\n",
        "bdt_cm = confusion_matrix(bdt_true, bdt_prediction)\n",
        "print('\\nDecision tree best model matrix:\\n', bdt_cm)\n",
        "\n",
        "# using the precision, the recall, and the f1-score to compare the decision trees model and the best decision trees model\n",
        "dt_overall = classification_report(dt_true, dt_prediction)\n",
        "print('\\nDecision tree overall\\n', dt_overall)\n",
        "bdt_overall = classification_report(bdt_true, bdt_prediction)\n",
        "print('\\nDecision tree best model overall:\\n', bdt_overall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el22OQZZQrrg"
      },
      "source": [
        "import timeit\n",
        "\n",
        "# LogisticRegression\n",
        "\n",
        "# --- LogisticRegression no tuning is done previously ---\n",
        "lr_accuracy = multi_evaluator.evaluate(predict_test, {multi_evaluator.metricName: \"accuracy\"}) \n",
        "\n",
        "# --- LogisticRegression with tuning ---\n",
        "#paramGrid = (ParamGridBuilder()\n",
        "#               .addGrid(lr.aggregationDepth,[2,5,10])\n",
        "#               .addGrid(lr.elasticNetParam,[0.0, 0.5, 1.0])\n",
        "#               .addGrid(lr.fitIntercept,[False, True])\n",
        "#               .addGrid(lr.maxIter,[5, 10, 100])\n",
        "#               .addGrid(lr.regParam,[0.01, 0.5, 2.0])\n",
        "#               .build())\n",
        "\n",
        "paramGrid = (ParamGridBuilder()\n",
        "               .addGrid(lr.maxIter,[5, 10, 100])\n",
        "               .addGrid(lr.aggregationDepth,[2,5,10])\n",
        "               .addGrid(lr.regParam,[0.01, 0.5, 2.0])\n",
        "               .build())\n",
        "\n",
        "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=multi_evaluator, numFolds=5)\n",
        "\n",
        "# Run cross validations.\n",
        "start = timeit.default_timer()\n",
        "\n",
        "cv_model = cv.fit(train)\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "print('Time: ', stop - start)  \n",
        "\n",
        "lr_model_best = cv_model.bestModel\n",
        "lr_predict_test_best = lr_model_best.transform(test)\n",
        "lr_accuracy_best = multi_evaluator.evaluate(lr_predict_test_best, {multi_evaluator.metricName: \"accuracy\"}) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPrgRFvhSIgv"
      },
      "source": [
        "# print comparison\n",
        "print('logistic regression accuracy:', round(lr_accuracy, 2))\n",
        "print('logistic best model:', round(lr_accuracy_best, 2))\n",
        "params = lr_model_best.extractParamMap()\n",
        "print('Logistic regression best model params', {param[0].name: param[1] for param in lr_model_best.extractParamMap().items()})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB_RgxjHBBOp"
      },
      "source": [
        "# using the confusion matrix to compare the decision trees model and the best decision trees model\n",
        "print('Logistic regression matrix:\\n', confusionMatrix)\n",
        "\n",
        "# LR best model\n",
        "blr_true = lr_predict_test_best.select(\"Target\").toPandas()\n",
        "blr_prediction = lr_predict_test_best.select(\"prediction\").toPandas()\n",
        "blr_cm = confusion_matrix(blr_true, blr_prediction)\n",
        "print('\\nLogistic regression best model matrix:\\n', blr_cm)\n",
        "\n",
        "# using the precision, the recall, and the f1-score to compare the decision trees model and the best decision trees model\n",
        "print('\\nLogistic regression overall:\\n', classificationReport)\n",
        "blr_overall = classification_report(blr_true, blr_prediction)\n",
        "print('\\nLogistic regression best model overall:\\n', bdt_overall)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK8KMdgbNYBI"
      },
      "source": [
        "# print accuracy comparison between models\n",
        "print('logistic regression accuracy:', round(lr_accuracy, 2))\n",
        "print('decision tree accuracy:', round(dt_accuracy, 2))\n",
        "print('random forest accuracy:', round(rf_accuracy, 2))\n",
        "\n",
        "# print hyperparameters of the chose model\n",
        "# according to this https://www.silect.is/blog/2019/4/2/random-forest-in-spark-ml\n",
        "# we choose which parameters worth of tuning it\n",
        "print(\"Random Forest best model params:\", {param[0].name: param[1] for param in rf_model_best.extractParamMap().items()})\n",
        "params = {param[0].name: param[1] for param in rf_model_best.extractParamMap().items()}\n",
        "# possible hyperparameters to be tuned: maxDepth, numTrees, maxBins, featureSubsetStrategy, minInfoGain , minInstancesPerNode \n",
        "# we only display these 3 because we only use these 3 in our paramGrids\n",
        "print('Best hyper-parameters for this model:')\n",
        "print('maxDepth:', params['maxDepth'])\n",
        "print('numTrees:', params['numTrees'])\n",
        "print('maxBins:', params['maxBins'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}